{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing | Visualizing | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will load our Fashion_MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 60000\n",
      "Number of labels in training data: 60000\n",
      "Dimensions of single image in our x-train data: (28, 28)\n",
      "------------------------------------------------------------------------\n",
      "Number of samples in test data: 10000\n",
      "Number of labels in test data: 10000\n",
      "Dimensions of single image in our x-test data: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "# We will explore the data set here\n",
    "# We will also check the shape and size of different loads\n",
    "\n",
    "print(\"Number of samples in training data: \" + str(len(x_train)))\n",
    "print(\"Number of labels in training data: \" + str(len(y_train)))\n",
    "print(\"Dimensions of single image in our x-train data: \" + str(x_train[0].shape))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Number of samples in test data: \" + str(len(x_test)))\n",
    "print(\"Number of labels in test data: \" + str(len(y_test)))\n",
    "print(\"Dimensions of single image in our x-test data: \" + str(x_test[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b4d8b6ea00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how we are gonna visualize images within the matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We will plot 5 images - Subplot arguments will represent nrows and ncols plus the index\n",
    "# We will be using a grey color map since our image set is grayscale\n",
    "\n",
    "plt.subplot(231)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(232)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(233)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(234)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(235)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(236)\n",
    "random_num = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[random_num], cmap = plt.get_cmap('gray'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Creating CNN Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,200,778\n",
      "Trainable params: 1,200,330\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Importing needed libraries \n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "# Here we will settle parameters and batch sizes plus our epochs\n",
    "batch_size = 128\n",
    "epochs = 150 # One of these is a forwards and backwards pass through the CNN which is why we divide it 100 times, which can make it more accurate\n",
    "\n",
    "# Need to store the number of rows and columns\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "\n",
    "''' We will be adding a 4th dimension to allow our Keras to get the data in the right 'shape'.\n",
    "Our shape image will go from the (60000, 28, 28) to (60000, 28, 28, 1)'''\n",
    "\n",
    "# The reason we use 1 as our channel is because the images we are using are grayscale\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) \n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# This will store the shape of a single image\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# This will change the image type to the float32 data-type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# We will normalize the data by changing the range from (0 - 255) to (0 - 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# This will perform our hot encoding \n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# This will calculate the number of classes and number of pixels\n",
    "num_classes = y_test.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# We are now creating a CNN Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation = 'relu',\n",
    "                 input_shape = input_shape))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 2.4576 - accuracy: 0.2828 - val_loss: 1.4766 - val_accuracy: 0.5249\n",
      "Epoch 2/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 1.4828 - accuracy: 0.5193 - val_loss: 0.8218 - val_accuracy: 0.7230\n",
      "Epoch 3/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 1.1839 - accuracy: 0.6102 - val_loss: 0.7218 - val_accuracy: 0.7518\n",
      "Epoch 4/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 1.0383 - accuracy: 0.6558 - val_loss: 0.6658 - val_accuracy: 0.7690\n",
      "Epoch 5/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.9562 - accuracy: 0.6815 - val_loss: 0.6268 - val_accuracy: 0.7801\n",
      "Epoch 6/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.8871 - accuracy: 0.7051 - val_loss: 0.5972 - val_accuracy: 0.7907\n",
      "Epoch 7/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.8451 - accuracy: 0.7164 - val_loss: 0.5741 - val_accuracy: 0.7970\n",
      "Epoch 8/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.8063 - accuracy: 0.7280 - val_loss: 0.5558 - val_accuracy: 0.8046\n",
      "Epoch 9/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.7747 - accuracy: 0.7388 - val_loss: 0.5403 - val_accuracy: 0.8099\n",
      "Epoch 10/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.7485 - accuracy: 0.7464 - val_loss: 0.5277 - val_accuracy: 0.8139\n",
      "Epoch 11/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.7295 - accuracy: 0.7513 - val_loss: 0.5177 - val_accuracy: 0.8172\n",
      "Epoch 12/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.7073 - accuracy: 0.7585 - val_loss: 0.5065 - val_accuracy: 0.8212\n",
      "Epoch 13/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.6864 - accuracy: 0.7661 - val_loss: 0.4972 - val_accuracy: 0.8242\n",
      "Epoch 14/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.6729 - accuracy: 0.7693 - val_loss: 0.4893 - val_accuracy: 0.8268\n",
      "Epoch 15/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.6636 - accuracy: 0.7735 - val_loss: 0.4825 - val_accuracy: 0.8294\n",
      "Epoch 16/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.6486 - accuracy: 0.7793 - val_loss: 0.4758 - val_accuracy: 0.8318\n",
      "Epoch 17/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.6418 - accuracy: 0.7805 - val_loss: 0.4692 - val_accuracy: 0.8349\n",
      "Epoch 18/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.6258 - accuracy: 0.7850 - val_loss: 0.4628 - val_accuracy: 0.8361\n",
      "Epoch 19/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.6172 - accuracy: 0.7880 - val_loss: 0.4582 - val_accuracy: 0.8384\n",
      "Epoch 20/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.6142 - accuracy: 0.7900 - val_loss: 0.4543 - val_accuracy: 0.8402\n",
      "Epoch 21/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.6004 - accuracy: 0.7932 - val_loss: 0.4492 - val_accuracy: 0.8429\n",
      "Epoch 22/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.5922 - accuracy: 0.7976 - val_loss: 0.4449 - val_accuracy: 0.8449\n",
      "Epoch 23/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.5857 - accuracy: 0.7994 - val_loss: 0.4416 - val_accuracy: 0.8458\n",
      "Epoch 24/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5813 - accuracy: 0.7998 - val_loss: 0.4379 - val_accuracy: 0.8462\n",
      "Epoch 25/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5753 - accuracy: 0.8033 - val_loss: 0.4337 - val_accuracy: 0.8481\n",
      "Epoch 26/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5641 - accuracy: 0.8079 - val_loss: 0.4305 - val_accuracy: 0.8499\n",
      "Epoch 27/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5600 - accuracy: 0.8098 - val_loss: 0.4274 - val_accuracy: 0.8502\n",
      "Epoch 28/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5529 - accuracy: 0.8110 - val_loss: 0.4242 - val_accuracy: 0.8514\n",
      "Epoch 29/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5563 - accuracy: 0.8091 - val_loss: 0.4218 - val_accuracy: 0.8517\n",
      "Epoch 30/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5457 - accuracy: 0.8123 - val_loss: 0.4187 - val_accuracy: 0.8539\n",
      "Epoch 31/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5425 - accuracy: 0.8145 - val_loss: 0.4157 - val_accuracy: 0.8540\n",
      "Epoch 32/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5369 - accuracy: 0.8162 - val_loss: 0.4132 - val_accuracy: 0.8546\n",
      "Epoch 33/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5323 - accuracy: 0.8172 - val_loss: 0.4111 - val_accuracy: 0.8559\n",
      "Epoch 34/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5269 - accuracy: 0.8199 - val_loss: 0.4086 - val_accuracy: 0.8566\n",
      "Epoch 35/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5236 - accuracy: 0.8207 - val_loss: 0.4062 - val_accuracy: 0.8569\n",
      "Epoch 36/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5204 - accuracy: 0.8212 - val_loss: 0.4041 - val_accuracy: 0.8583\n",
      "Epoch 37/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5150 - accuracy: 0.8241 - val_loss: 0.4015 - val_accuracy: 0.8593\n",
      "Epoch 38/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5107 - accuracy: 0.8240 - val_loss: 0.3996 - val_accuracy: 0.8600\n",
      "Epoch 39/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5103 - accuracy: 0.8252 - val_loss: 0.3977 - val_accuracy: 0.8605\n",
      "Epoch 40/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.5044 - accuracy: 0.8278 - val_loss: 0.3961 - val_accuracy: 0.8606\n",
      "Epoch 41/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.5006 - accuracy: 0.8285 - val_loss: 0.3940 - val_accuracy: 0.8613\n",
      "Epoch 42/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.5005 - accuracy: 0.8273 - val_loss: 0.3923 - val_accuracy: 0.8621\n",
      "Epoch 43/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4963 - accuracy: 0.8300 - val_loss: 0.3903 - val_accuracy: 0.8627\n",
      "Epoch 44/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4908 - accuracy: 0.8311 - val_loss: 0.3884 - val_accuracy: 0.8633\n",
      "Epoch 45/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4921 - accuracy: 0.8302 - val_loss: 0.3866 - val_accuracy: 0.8645\n",
      "Epoch 46/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4857 - accuracy: 0.8341 - val_loss: 0.3846 - val_accuracy: 0.8648\n",
      "Epoch 47/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4878 - accuracy: 0.8324 - val_loss: 0.3835 - val_accuracy: 0.8645\n",
      "Epoch 48/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4801 - accuracy: 0.8348 - val_loss: 0.3814 - val_accuracy: 0.8651\n",
      "Epoch 49/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4797 - accuracy: 0.8359 - val_loss: 0.3803 - val_accuracy: 0.8658\n",
      "Epoch 50/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4800 - accuracy: 0.8365 - val_loss: 0.3787 - val_accuracy: 0.8651\n",
      "Epoch 51/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4771 - accuracy: 0.8366 - val_loss: 0.3771 - val_accuracy: 0.8662\n",
      "Epoch 52/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4696 - accuracy: 0.8385 - val_loss: 0.3758 - val_accuracy: 0.8669\n",
      "Epoch 53/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4713 - accuracy: 0.8365 - val_loss: 0.3742 - val_accuracy: 0.8679\n",
      "Epoch 54/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4639 - accuracy: 0.8402 - val_loss: 0.3732 - val_accuracy: 0.8676\n",
      "Epoch 55/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4675 - accuracy: 0.8400 - val_loss: 0.3720 - val_accuracy: 0.8682\n",
      "Epoch 56/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.4644 - accuracy: 0.8412 - val_loss: 0.3706 - val_accuracy: 0.8687\n",
      "Epoch 57/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4592 - accuracy: 0.8421 - val_loss: 0.3698 - val_accuracy: 0.8693\n",
      "Epoch 58/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.4595 - accuracy: 0.8399 - val_loss: 0.3678 - val_accuracy: 0.8691\n",
      "Epoch 59/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4562 - accuracy: 0.8415 - val_loss: 0.3667 - val_accuracy: 0.8699\n",
      "Epoch 60/150\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.4591 - accuracy: 0.8411 - val_loss: 0.3661 - val_accuracy: 0.8700\n",
      "Epoch 61/150\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.4520 - accuracy: 0.8432 - val_loss: 0.3648 - val_accuracy: 0.8703\n",
      "Epoch 62/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4502 - accuracy: 0.8437 - val_loss: 0.3638 - val_accuracy: 0.8709\n",
      "Epoch 63/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4485 - accuracy: 0.8448 - val_loss: 0.3622 - val_accuracy: 0.8714\n",
      "Epoch 64/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4457 - accuracy: 0.8472 - val_loss: 0.3613 - val_accuracy: 0.8723\n",
      "Epoch 65/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4474 - accuracy: 0.8431 - val_loss: 0.3605 - val_accuracy: 0.8721\n",
      "Epoch 66/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4468 - accuracy: 0.8463 - val_loss: 0.3594 - val_accuracy: 0.8724\n",
      "Epoch 67/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4413 - accuracy: 0.8469 - val_loss: 0.3581 - val_accuracy: 0.8729\n",
      "Epoch 68/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4391 - accuracy: 0.8497 - val_loss: 0.3573 - val_accuracy: 0.8731\n",
      "Epoch 69/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4393 - accuracy: 0.8482 - val_loss: 0.3561 - val_accuracy: 0.8729\n",
      "Epoch 70/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4395 - accuracy: 0.8480 - val_loss: 0.3550 - val_accuracy: 0.8740\n",
      "Epoch 71/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4352 - accuracy: 0.8511 - val_loss: 0.3539 - val_accuracy: 0.8743\n",
      "Epoch 72/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4355 - accuracy: 0.8494 - val_loss: 0.3529 - val_accuracy: 0.8748\n",
      "Epoch 73/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4302 - accuracy: 0.8519 - val_loss: 0.3522 - val_accuracy: 0.8746\n",
      "Epoch 74/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4292 - accuracy: 0.8528 - val_loss: 0.3511 - val_accuracy: 0.8756\n",
      "Epoch 75/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4273 - accuracy: 0.8505 - val_loss: 0.3499 - val_accuracy: 0.8761\n",
      "Epoch 76/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4272 - accuracy: 0.8521 - val_loss: 0.3491 - val_accuracy: 0.8757\n",
      "Epoch 77/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4277 - accuracy: 0.8533 - val_loss: 0.3482 - val_accuracy: 0.8764\n",
      "Epoch 78/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4241 - accuracy: 0.8547 - val_loss: 0.3475 - val_accuracy: 0.8760\n",
      "Epoch 79/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4243 - accuracy: 0.8530 - val_loss: 0.3469 - val_accuracy: 0.8766\n",
      "Epoch 80/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4223 - accuracy: 0.8534 - val_loss: 0.3457 - val_accuracy: 0.8765\n",
      "Epoch 81/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4182 - accuracy: 0.8552 - val_loss: 0.3448 - val_accuracy: 0.8767\n",
      "Epoch 82/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4186 - accuracy: 0.8558 - val_loss: 0.3440 - val_accuracy: 0.8768\n",
      "Epoch 83/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.4169 - accuracy: 0.8561 - val_loss: 0.3435 - val_accuracy: 0.8765\n",
      "Epoch 84/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4152 - accuracy: 0.8566 - val_loss: 0.3426 - val_accuracy: 0.8774\n",
      "Epoch 85/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4127 - accuracy: 0.8575 - val_loss: 0.3419 - val_accuracy: 0.8775\n",
      "Epoch 86/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4164 - accuracy: 0.8556 - val_loss: 0.3412 - val_accuracy: 0.8778\n",
      "Epoch 87/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4099 - accuracy: 0.8582 - val_loss: 0.3407 - val_accuracy: 0.8778\n",
      "Epoch 88/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4121 - accuracy: 0.8574 - val_loss: 0.3394 - val_accuracy: 0.8780\n",
      "Epoch 89/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.4088 - accuracy: 0.8580 - val_loss: 0.3386 - val_accuracy: 0.8782\n",
      "Epoch 90/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4056 - accuracy: 0.8606 - val_loss: 0.3380 - val_accuracy: 0.8782\n",
      "Epoch 91/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4092 - accuracy: 0.8584 - val_loss: 0.3373 - val_accuracy: 0.8790\n",
      "Epoch 92/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4066 - accuracy: 0.8590 - val_loss: 0.3364 - val_accuracy: 0.8790\n",
      "Epoch 93/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4028 - accuracy: 0.8611 - val_loss: 0.3356 - val_accuracy: 0.8790\n",
      "Epoch 94/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4059 - accuracy: 0.8597 - val_loss: 0.3353 - val_accuracy: 0.8791\n",
      "Epoch 95/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3998 - accuracy: 0.8611 - val_loss: 0.3349 - val_accuracy: 0.8797\n",
      "Epoch 96/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.4004 - accuracy: 0.8616 - val_loss: 0.3339 - val_accuracy: 0.8793\n",
      "Epoch 97/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3986 - accuracy: 0.8612 - val_loss: 0.3333 - val_accuracy: 0.8797\n",
      "Epoch 98/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3983 - accuracy: 0.8626 - val_loss: 0.3324 - val_accuracy: 0.8795\n",
      "Epoch 99/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.3949 - accuracy: 0.8630 - val_loss: 0.3317 - val_accuracy: 0.8804\n",
      "Epoch 100/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3990 - accuracy: 0.8624 - val_loss: 0.3310 - val_accuracy: 0.8800\n",
      "Epoch 101/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3963 - accuracy: 0.8626 - val_loss: 0.3301 - val_accuracy: 0.8809\n",
      "Epoch 102/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3956 - accuracy: 0.8625 - val_loss: 0.3299 - val_accuracy: 0.8806\n",
      "Epoch 103/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3936 - accuracy: 0.8630 - val_loss: 0.3293 - val_accuracy: 0.8814\n",
      "Epoch 104/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3911 - accuracy: 0.8650 - val_loss: 0.3286 - val_accuracy: 0.8809\n",
      "Epoch 105/150\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.3936 - accuracy: 0.8630 - val_loss: 0.3276 - val_accuracy: 0.8821\n",
      "Epoch 106/150\n",
      "469/469 [==============================] - 40s 84ms/step - loss: 0.3890 - accuracy: 0.8660 - val_loss: 0.3273 - val_accuracy: 0.8816\n",
      "Epoch 107/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3899 - accuracy: 0.8660 - val_loss: 0.3265 - val_accuracy: 0.8818\n",
      "Epoch 108/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3911 - accuracy: 0.8657 - val_loss: 0.3262 - val_accuracy: 0.8818\n",
      "Epoch 109/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3893 - accuracy: 0.8654 - val_loss: 0.3254 - val_accuracy: 0.8824\n",
      "Epoch 110/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3854 - accuracy: 0.8665 - val_loss: 0.3248 - val_accuracy: 0.8819\n",
      "Epoch 111/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3817 - accuracy: 0.8674 - val_loss: 0.3244 - val_accuracy: 0.8828\n",
      "Epoch 112/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3823 - accuracy: 0.8675 - val_loss: 0.3235 - val_accuracy: 0.8827\n",
      "Epoch 113/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3814 - accuracy: 0.8673 - val_loss: 0.3230 - val_accuracy: 0.8832\n",
      "Epoch 114/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3794 - accuracy: 0.8677 - val_loss: 0.3226 - val_accuracy: 0.8831\n",
      "Epoch 115/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3798 - accuracy: 0.8677 - val_loss: 0.3220 - val_accuracy: 0.8844\n",
      "Epoch 116/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3801 - accuracy: 0.8697 - val_loss: 0.3214 - val_accuracy: 0.8838\n",
      "Epoch 117/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3817 - accuracy: 0.8672 - val_loss: 0.3210 - val_accuracy: 0.8850\n",
      "Epoch 118/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3801 - accuracy: 0.8695 - val_loss: 0.3202 - val_accuracy: 0.8853\n",
      "Epoch 119/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3757 - accuracy: 0.8709 - val_loss: 0.3200 - val_accuracy: 0.8850\n",
      "Epoch 120/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3752 - accuracy: 0.8710 - val_loss: 0.3190 - val_accuracy: 0.8851\n",
      "Epoch 121/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3743 - accuracy: 0.8706 - val_loss: 0.3186 - val_accuracy: 0.8852\n",
      "Epoch 122/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3744 - accuracy: 0.8698 - val_loss: 0.3182 - val_accuracy: 0.8858\n",
      "Epoch 123/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3727 - accuracy: 0.8700 - val_loss: 0.3174 - val_accuracy: 0.8866\n",
      "Epoch 124/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3698 - accuracy: 0.8716 - val_loss: 0.3169 - val_accuracy: 0.8869\n",
      "Epoch 125/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3704 - accuracy: 0.8697 - val_loss: 0.3165 - val_accuracy: 0.8871\n",
      "Epoch 126/150\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.3698 - accuracy: 0.8726 - val_loss: 0.3160 - val_accuracy: 0.8875\n",
      "Epoch 127/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3697 - accuracy: 0.8719 - val_loss: 0.3153 - val_accuracy: 0.8879\n",
      "Epoch 128/150\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.3692 - accuracy: 0.8724 - val_loss: 0.3149 - val_accuracy: 0.8878\n",
      "Epoch 129/150\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3687 - accuracy: 0.8720 - val_loss: 0.3146 - val_accuracy: 0.8883\n",
      "Epoch 130/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.3650 - accuracy: 0.8731 - val_loss: 0.3140 - val_accuracy: 0.8880\n",
      "Epoch 131/150\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.3681 - accuracy: 0.8724 - val_loss: 0.3134 - val_accuracy: 0.8885\n",
      "Epoch 132/150\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.3686 - accuracy: 0.8717 - val_loss: 0.3131 - val_accuracy: 0.8880\n",
      "Epoch 133/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3630 - accuracy: 0.8748 - val_loss: 0.3129 - val_accuracy: 0.8887\n",
      "Epoch 134/150\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.3636 - accuracy: 0.8748 - val_loss: 0.3126 - val_accuracy: 0.8889\n",
      "Epoch 135/150\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.3640 - accuracy: 0.8739 - val_loss: 0.3118 - val_accuracy: 0.8889\n",
      "Epoch 136/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3630 - accuracy: 0.8739 - val_loss: 0.3115 - val_accuracy: 0.8889\n",
      "Epoch 137/150\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.3627 - accuracy: 0.8739 - val_loss: 0.3109 - val_accuracy: 0.8896\n",
      "Epoch 138/150\n",
      "469/469 [==============================] - 40s 86ms/step - loss: 0.3584 - accuracy: 0.8746 - val_loss: 0.3101 - val_accuracy: 0.8897\n",
      "Epoch 139/150\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.3556 - accuracy: 0.8761 - val_loss: 0.3100 - val_accuracy: 0.8893\n",
      "Epoch 140/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3598 - accuracy: 0.8751 - val_loss: 0.3094 - val_accuracy: 0.8893\n",
      "Epoch 141/150\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.3619 - accuracy: 0.8741 - val_loss: 0.3091 - val_accuracy: 0.8896\n",
      "Epoch 142/150\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.3572 - accuracy: 0.8774 - val_loss: 0.3087 - val_accuracy: 0.8899\n",
      "Epoch 143/150\n",
      "469/469 [==============================] - 40s 86ms/step - loss: 0.3578 - accuracy: 0.8747 - val_loss: 0.3082 - val_accuracy: 0.8899\n",
      "Epoch 144/150\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.3569 - accuracy: 0.8763 - val_loss: 0.3080 - val_accuracy: 0.8896\n",
      "Epoch 145/150\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.3553 - accuracy: 0.8772 - val_loss: 0.3075 - val_accuracy: 0.8897\n",
      "Epoch 146/150\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.3538 - accuracy: 0.8767 - val_loss: 0.3070 - val_accuracy: 0.8905\n",
      "Epoch 147/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3535 - accuracy: 0.8778 - val_loss: 0.3066 - val_accuracy: 0.8905\n",
      "Epoch 148/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3491 - accuracy: 0.8790 - val_loss: 0.3061 - val_accuracy: 0.8910\n",
      "Epoch 149/150\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.3527 - accuracy: 0.8774 - val_loss: 0.3058 - val_accuracy: 0.8908\n",
      "Epoch 150/150\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.3503 - accuracy: 0.8772 - val_loss: 0.3055 - val_accuracy: 0.8908\n",
      "Test Loss:  0.3054705262184143\n",
      "Test Accuracy:  0.8907999992370605\n"
     ]
    }
   ],
   "source": [
    "model_fitting = model.fit(x_train, y_train,\n",
    "                          batch_size = batch_size, # 128 images per batch\n",
    "                          epochs = epochs, # More time to take to train the model\n",
    "                          verbose = 1, # How much information we wanna see. . . 1->will show us the progress bar\n",
    "                          validation_data = (x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('image_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_image(filename):\n",
    "    # Load the image\n",
    "    img = tf.keras.utils.load_img(filename, grayscale=True, target_size=(28, 28))\n",
    "    \n",
    "    # Converting the image to array\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "    \n",
    "    # Reshape the image into a sample of 1 channel\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "    \n",
    "    # Prepare this as pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
